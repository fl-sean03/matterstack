# Evidence Bundles (Results & Reproducibility)

In scientific computing, the "result" is not just the final number; it is the entire chain of custody that produced it. MatterStack treats the output of every campaign as an **Evidence Bundle**—a structured, self-contained archive of the execution.

## Directory Structure

When a campaign runs, it writes to a `results/` directory (or a custom path defined by the backend). The structure is strictly hierarchical:

```text
results/
├── task_id_1/
│   ├── stdout.log         # Standard output stream
│   ├── stderr.log         # Standard error stream
│   ├── submit.sh          # The exact script submitted to Slurm (for audit)
│   ├── input.json         # Input files created by the task definition
│   └── output.data        # Artifacts generated by the scientific code
├── task_id_2/
│   └── ...
└── workflow_summary.json  # Metadata about the entire run
```

## Reproducibility by Design

### 1. The `submit.sh` Audit Trail
For HPC runs, MatterStack generates and saves the exact batch script used. This allows researchers to verify exactly what modules were loaded, what environment variables were set, and what commands were executed.

### 2. Containerization
Tasks specify container images (`image: "ubuntu:22.04"`). While `LocalBackend` may run natively, `SlurmBackend` is designed to map these to Singularity/Apptainer images, ensuring that the software environment is frozen and reproducible.

### 3. Captured Streams
`stdout` and `stderr` are always captured. This ensures that even if the scientific code crashes or prints a warning, the evidence is preserved for debugging.

## Parsing Results

The Campaign Engine's `analyze()` phase programmatically reads these bundles.

```python
# In Campaign.analyze(result)
for task_id, task_result in result.tasks.items():
    if task_result.status.state == "COMPLETED":
        # Read a specific output file
        output_path = task_result.workspace_path / "results.json"
        data = json.loads(output_path.read_text())
        
        # Or parse stdout
        log = task_result.logs.stdout
```

## Archival

Because the Evidence Bundle is just a directory of files, it can be easily:
*   Zipped and attached to a publication.
*   Uploaded to a data repository (Zenodo, Figshare).
*   Indexed by a search engine (using the JSON metadata).